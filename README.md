![Alt text](./EG_LOGO.png)
# Synthetic Review Generator with Guardrails

This project implements a **synthetic data generation pipeline** for realistic software/tool reviews, enriched with **automated quality guardrails** and **model-level comparison**.  
The system is designed to generate human-like reviews, evaluate their quality, enforce diversity, and compare multiple LLMs under identical conditions.

---

## ğŸš€ Project Overview

The goal of this project is to generate **high-quality synthetic user reviews** while avoiding:
- Near-duplicate content
- Unrealistic or marketing-style language
- Domain-irrelevant text
- Sentimentâ€“rating mismatch

To achieve this, we combine:
- Controlled generation via prompts and model parameters
- Semantic evaluation using sentence embeddings
- Automatic rejection and regeneration
- Comparative evaluation between multiple LLMs

---

## âœ¨ Key Features

- **Synthetic Review Generation**
  - Persona-aware, rating-controlled review generation
  - Supports multiple LLMs (Groq-hosted LLaMA models)
  - Configurable via JSON

- **Quality Guardrails**
  - Semantic diversity enforcement
  - Domain realism validation
  - Exaggeration / marketing-hype detection
  - Length and rating-consistency checks
  - Composite quality score (0â€“1)

- **Automatic Regeneration**
  - Low-quality samples are rejected and regenerated
  - Configurable retry limits

- **Model Comparison**
  - Compare quality, diversity, and performance across models
  - Generates a structured Markdown report

- **FastAPI Interface**
  - Downloadable quality report (`.md`)
  - Ready for extension (API / CLI)

---

## ğŸ§  Models Used

The project compares two Groq-hosted models under identical conditions:

- **LLaMA 3.3 70B â€“ Versatile**
- **LLaMA 4 Scout 17B â€“ Instruct**

Both models:
- Use the same prompts
- Use the same generation parameters
- Are evaluated using the same quality pipeline

---

## ğŸ—ï¸ Architecture Overview

```text
Generation
   â†“
Prompt + Model Parameters
   â†“
Raw Review
   â†“
Quality Evaluation
   â”œâ”€ Length Check
   â”œâ”€ Semantic Diversity (Sentence Transformers)
   â”œâ”€ Domain Semantic Similarity
   â”œâ”€ Rating Consistency
   â”œâ”€ Exaggeration Detection
   â†“
Quality Score (0â€“1)
   â†“
Accept / Reject
   â†“
Dataset + Metrics
   â†“
Model Comparison Report (.md)
```


## ğŸ—ï¸ Architecture Overview

```
â”œâ”€â”€ config
â”‚Â Â  â”œâ”€â”€ config.json
â”‚Â Â  â””â”€â”€ __init__.py
â”œâ”€â”€ generate_reports
â”‚Â Â  â”œâ”€â”€ generate_reports.py
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â””â”€â”€ __pycache__
â”‚Â Â      â”œâ”€â”€ generate_reports.cpython-310.pyc
â”‚Â Â      â””â”€â”€ __init__.cpython-310.pyc
â”œâ”€â”€ helpers
â”‚Â Â  â”œâ”€â”€ compare_models.py
â”‚Â Â  â”œâ”€â”€ compare_real.py
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ __pycache__
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ compare_models.cpython-310.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ compare_real.cpython-310.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.cpython-310.pyc
â”‚Â Â  â”‚Â Â  â””â”€â”€ utils.cpython-310.pyc
â”‚Â Â  â””â”€â”€ utils.py
â”œâ”€â”€ llm_store
â”‚Â Â  â”œâ”€â”€ groq.py
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â””â”€â”€ __pycache__
â”‚Â Â      â”œâ”€â”€ groq.cpython-310.pyc
â”‚Â Â      â””â”€â”€ __init__.cpython-310.pyc
â”œâ”€â”€ main.py
â”œâ”€â”€ output
â”‚Â Â  â”œâ”€â”€ llama-4-scout-17b-16e-instruct.json
â”‚Â Â  â”œâ”€â”€ real_reviews.json
â”‚Â Â  â””â”€â”€ reviews_llama-3.3-70b-versatile.json
â”œâ”€â”€ __pycache__
â”‚Â Â  â””â”€â”€ main.cpython-310.pyc
â”œâ”€â”€ README.md
â”œâ”€â”€ reports
â”‚Â Â  â””â”€â”€ report.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ review
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ __pycache__
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.cpython-310.pyc
â”‚Â Â  â”‚Â Â  â””â”€â”€ review_generated_sample.cpython-310.pyc
â”‚Â Â  â””â”€â”€ review_generated_sample.py
â””â”€â”€ routes
    â”œâ”€â”€ generate.py
    â”œâ”€â”€ generate_reports_api.py
    â”œâ”€â”€ __init__.py
    â””â”€â”€ __pycache__
        â”œâ”€â”€ generate.cpython-310.pyc
        â”œâ”€â”€ generate_reports_api.cpython-310.pyc
        â””â”€â”€ __init__.cpython-310.pyc

15 directories, 36 files
```

## ğŸ“Š Quality Scoring

Each review receives a **composite quality score** based on:

| Component          | Description                            |
| ------------------ | -------------------------------------- |
| Length             | Compliance with min/max word limits    |
| Semantic Diversity | Penalizes near-duplicate content       |
| Domain Realism     | Semantic similarity to domain anchor   |
| Rating Consistency | Alignment between text and rating      |
| Forbidden Penalty  | Penalizes exaggerated / marketing tone |

### Final Quality Formula

```text
quality =
(0.20 Ã— length
+ 0.35 Ã— diversity
+ 0.25 Ã— domain
+ 0.20 Ã— rating)
Ã— penalty
```

## ğŸ§¬ Semantic Evaluation

* Uses **Sentence Transformers (MiniLM)** on CPU for portability
* All embeddings are normalized
* Cosine similarity is used for:

  * Inter-review similarity (diversity)
  * Domain realism scoring
  * Exaggeration detection (anchor-based)

This avoids brittle keyword-based heuristics.

## ğŸ“ˆ Model Comparison

The system compares models on:

### Performance

* Average generation time
* Median generation time
* Attempts per accepted review

### Quality

* Average quality score
* Quality variance (consistency)
* Acceptance rate

### Diversity

* Average semantic similarity
* Near-duplicate rate
* Average diversity score

### Domain Realism

* Domain semantic score
* Forbidden / exaggeration score
