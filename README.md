![Alt text](./EG_LOGO.png)
# Synthetic Review Generator with Guardrails

This project implements a **synthetic data generation pipeline** for realistic software/tool reviews, enriched with **automated quality guardrails** and **model-level comparison**.  
The system is designed to generate human-like reviews, evaluate their quality, enforce diversity, and compare multiple LLMs under identical conditions.

---

## ğŸš€ Project Overview

The goal of this project is to generate **high-quality synthetic user reviews** while avoiding:
- Near-duplicate content
- Unrealistic or marketing-style language
- Domain-irrelevant text
- Sentimentâ€“rating mismatch

To achieve this, we combine:
- Controlled generation via prompts and model parameters
- Semantic evaluation using sentence embeddings
- Automatic rejection and regeneration
- Comparative evaluation between multiple LLMs

---

## âœ¨ Key Features

- **Synthetic Review Generation**
  - Persona-aware, rating-controlled review generation
  - Supports multiple LLMs (Groq-hosted LLaMA models)
  - Configurable via JSON

- **Quality Guardrails**
  - Semantic diversity enforcement
  - Domain realism validation
  - Exaggeration / marketing-hype detection
  - Length and rating-consistency checks
  - Composite quality score (0â€“1)

- **Automatic Regeneration**
  - Low-quality samples are rejected and regenerated
  - Configurable retry limits

- **Model Comparison**
  - Compare quality, diversity, and performance across models
  - Generates a structured Markdown report

- **FastAPI Interface**
  - Downloadable quality report (`.md`)
  - Ready for extension (API / CLI)

---

## ğŸ§  Models Used

The project compares two Groq-hosted models under identical conditions:

- **LLaMA 3.3 70B â€“ Versatile**
- **LLaMA 4 Scout 17B â€“ Instruct**

Both models:
- Use the same prompts
- Use the same generation parameters
- Are evaluated using the same quality pipeline

---

## ğŸ—ï¸ Architecture Overview

```text
Generation
   â†“
Prompt + Model Parameters
   â†“
Raw Review
   â†“
Quality Evaluation
   â”œâ”€ Length Check
   â”œâ”€ Semantic Diversity (Sentence Transformers)
   â”œâ”€ Domain Semantic Similarity
   â”œâ”€ Rating Consistency
   â”œâ”€ Exaggeration Detection
   â†“
Quality Score (0â€“1)
   â†“
Accept / Reject
   â†“
Dataset + Metrics
   â†“
Model Comparison Report (.md)
```


## ğŸ—ï¸ Architecture Overview

```
â”œâ”€â”€ config
â”‚Â Â  â”œâ”€â”€ config.json
â”‚Â Â  â””â”€â”€ __init__.py
â”œâ”€â”€ generate_reports
â”‚Â Â  â”œâ”€â”€ generate_reports.py
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â””â”€â”€ __pycache__
â”‚Â Â      â”œâ”€â”€ generate_reports.cpython-310.pyc
â”‚Â Â      â””â”€â”€ __init__.cpython-310.pyc
â”œâ”€â”€ helpers
â”‚Â Â  â”œâ”€â”€ compare_models.py
â”‚Â Â  â”œâ”€â”€ compare_real.py
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ __pycache__
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ compare_models.cpython-310.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ compare_real.cpython-310.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.cpython-310.pyc
â”‚Â Â  â”‚Â Â  â””â”€â”€ utils.cpython-310.pyc
â”‚Â Â  â””â”€â”€ utils.py
â”œâ”€â”€ llm_store
â”‚Â Â  â”œâ”€â”€ groq.py
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â””â”€â”€ __pycache__
â”‚Â Â      â”œâ”€â”€ groq.cpython-310.pyc
â”‚Â Â      â””â”€â”€ __init__.cpython-310.pyc
â”œâ”€â”€ main.py
â”œâ”€â”€ output
â”‚Â Â  â”œâ”€â”€ llama-4-scout-17b-16e-instruct.json
â”‚Â Â  â”œâ”€â”€ real_reviews.json
â”‚Â Â  â””â”€â”€ reviews_llama-3.3-70b-versatile.json
â”œâ”€â”€ __pycache__
â”‚Â Â  â””â”€â”€ main.cpython-310.pyc
â”œâ”€â”€ README.md
â”œâ”€â”€ reports
â”‚Â Â  â””â”€â”€ report.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ review
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ __pycache__
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.cpython-310.pyc
â”‚Â Â  â”‚Â Â  â””â”€â”€ review_generated_sample.cpython-310.pyc
â”‚Â Â  â””â”€â”€ review_generated_sample.py
â””â”€â”€ routes
    â”œâ”€â”€ generate.py
    â”œâ”€â”€ generate_reports_api.py
    â”œâ”€â”€ __init__.py
    â””â”€â”€ __pycache__
        â”œâ”€â”€ generate.cpython-310.pyc
        â”œâ”€â”€ generate_reports_api.cpython-310.pyc
        â””â”€â”€ __init__.cpython-310.pyc

15 directories, 36 files
```

## ğŸ“Š Quality Scoring

Each review receives a **composite quality score** based on:

| Component          | Description                            |
| ------------------ | -------------------------------------- |
| Length             | Compliance with min/max word limits    |
| Semantic Diversity | Penalizes near-duplicate content       |
| Domain Realism     | Semantic similarity to domain anchor   |
| Rating Consistency | Alignment between text and rating      |
| Forbidden Penalty  | Penalizes exaggerated / marketing tone |

### Final Quality Formula

```text
quality =
(0.20 Ã— length
+ 0.35 Ã— diversity
+ 0.25 Ã— domain
+ 0.20 Ã— rating)
Ã— penalty
```

## ğŸ§¬ Semantic Evaluation

* Uses **Sentence Transformers (MiniLM)** on CPU for portability
* All embeddings are normalized
* Cosine similarity is used for:

  * Inter-review similarity (diversity)
  * Domain realism scoring
  * Exaggeration detection (anchor-based)

This avoids brittle keyword-based heuristics.

## ğŸ“ˆ Model Comparison

The system compares models on:

### Performance

* Average generation time
* Median generation time
* Attempts per accepted review

### Quality

* Average quality score
* Quality variance (consistency)
* Acceptance rate

### Diversity

* Average semantic similarity
* Near-duplicate rate
* Average diversity score

### Domain Realism

* Domain semantic score
* Forbidden / exaggeration score

## ğŸ³ Docker Support

The project is fully containerized using **Docker** to ensure reproducibility, portability, and ease of deployment across different environments.

The Docker setup is **CPU-only** and optimized for lightweight NLP workloads, making it suitable for local development, servers, and cloud environments without GPU requirements.

---

## ğŸ“¦ Docker Requirements

* Docker >= 20.x
* No GPU or CUDA required
* Internet access on first run (to download models)

---

## ğŸ—ï¸ Docker Image Design

* **Base image:** `python:3.10-slim`
* **Execution mode:** CPU-only
* **Web server:** Uvicorn
* **Framework:** FastAPI
* **Semantic models:** Sentence Transformers (MiniLM)

The image is intentionally kept minimal to reduce size and startup time.

---

## ğŸ› ï¸ Build Docker Image

From the project root directory:

```bash
docker build -t synthetic-review-api .
```

This will:

* Install all Python dependencies
* Copy the full project source
* Prepare the FastAPI application for execution

---

## â–¶ï¸ Run the Container

### Basic run

```bash
docker run -p 8000:8000 synthetic-review-api
```

Access the API at:

```
http://localhost:8000
```

Swagger documentation:

```
http://localhost:8000/docs
```

---

## ğŸ“‚ Persisting Outputs (Recommended)

To persist generated reports and outputs outside the container:

```bash
docker run \
  -p 8000:8000 \
  -v $(pwd)/reports:/app/reports \
  -v $(pwd)/output:/app/output \
  synthetic-review-api
```

This ensures:

* Generated Markdown reports are saved locally
* Output datasets are preserved across container restarts

---

## ğŸ“„ Downloading the Report (Docker)

Once the container is running, the comparison report can be downloaded via:

```http
POST /generate_reports
```

The response is a downloadable Markdown file:

```
model_comparison.md
```

---

## âš™ï¸ Environment & Performance Notes

* Sentence Transformer models are downloaded on first run and cached inside the container
* Initial startup may take slightly longer due to model loading
* Subsequent requests are significantly faster
* The system is optimized for datasets in the range of hundreds to low thousands of samples

---

## ğŸ”’ Security & Best Practices

* No dynamic file paths are exposed in the API
* Download endpoints are read-only
* Docker image excludes unnecessary files via `.dockerignore`
* CPU-only execution avoids GPU dependency issues

---

## ğŸ“Œ Design Rationale

Docker was chosen to:

* Standardize the execution environment
* Simplify deployment and evaluation
* Avoid dependency conflicts
* Ensure consistent benchmarking across models

---

## ğŸ§¾ Example README Footer Addition (Optional)

```md
This project is fully containerized and can be deployed on any Docker-compatible environment without additional system dependencies.
```

---

## âœ… Summary of Docker Integration

* âœ” Portable deployment
* âœ” CPU-only (no CUDA required)
* âœ” FastAPI-ready
* âœ” Persistent outputs supported
* âœ” Production-friendly setup

---
